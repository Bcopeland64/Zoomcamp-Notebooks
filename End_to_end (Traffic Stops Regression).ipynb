{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('/home/brandon/IU International University/Project: From Model to Production/opp-stops_state.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the data\n",
    "summary = data.describe(include='all').transpose()\n",
    "summary['missing_values'] = data.isnull().sum()\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for numerical columns\n",
    "data[['search_rate', 'stop_rate', 'hit_rate', 'inferred_threshold', 'stops_per_year', 'stop_rate_n']].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 20))\n",
    "\n",
    "# Distribution of stops by race\n",
    "sns.countplot(ax=axes[0], data=data, x='subject_race')\n",
    "axes[0].set_title('Distribution of Stops by Race')\n",
    "\n",
    "# Stop rate by race\n",
    "sns.boxplot(ax=axes[1], data=data, x='subject_race', y='stop_rate')\n",
    "axes[1].set_title('Stop Rate by Race')\n",
    "\n",
    "# Search rate by race\n",
    "sns.boxplot(ax=axes[2], data=data, x='subject_race', y='search_rate')\n",
    "axes[2].set_title('Search Rate by Race')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 20))\n",
    "\n",
    "# Total stops per state\n",
    "total_stops_per_state = data.groupby('state')['stops_per_year'].sum().sort_values(ascending=False)\n",
    "sns.barplot(ax=axes[0], x=total_stops_per_state.index, y=total_stops_per_state.values)\n",
    "axes[0].set_title('Total Stops per State')\n",
    "axes[0].set_xlabel('State')\n",
    "axes[0].set_ylabel('Total Stops per Year')\n",
    "\n",
    "# Average stop rate per state\n",
    "avg_stop_rate_per_state = data.groupby('state')['stop_rate'].mean().sort_values(ascending=False)\n",
    "sns.barplot(ax=axes[1], x=avg_stop_rate_per_state.index, y=avg_stop_rate_per_state.values)\n",
    "axes[1].set_title('Average Stop Rate per State')\n",
    "axes[1].set_xlabel('State')\n",
    "axes[1].set_ylabel('Average Stop Rate')\n",
    "\n",
    "# Average search rate per state\n",
    "avg_search_rate_per_state = data.groupby('state')['search_rate'].mean().sort_values(ascending=False)\n",
    "sns.barplot(ax=axes[2], x=avg_search_rate_per_state.index, y=avg_search_rate_per_state.values)\n",
    "axes[2].set_title('Average Search Rate per State')\n",
    "axes[2].set_xlabel('State')\n",
    "axes[2].set_ylabel('Average Search Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of stop rate vs search rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=data, x='stop_rate', y='search_rate')\n",
    "plt.title('Stop Rate vs Search Rate')\n",
    "plt.xlabel('Stop Rate')\n",
    "plt.ylabel('Search Rate')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "data[['stop_rate', 'search_rate']].corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of hit rate by race\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=data, x='subject_race', y='hit_rate')\n",
    "plt.title('Hit Rate by Race')\n",
    "plt.xlabel('Race')\n",
    "plt.ylabel('Hit Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 counties with the highest number of stops\n",
    "top_stops_counties = data.groupby('subgeography')['stops_per_year'].sum().nlargest(10)\n",
    "\n",
    "# Top 10 counties with the highest stop rate\n",
    "top_stoprate_counties = data.groupby('subgeography')['stop_rate'].mean().nlargest(10)\n",
    "\n",
    "# Top 10 counties with the highest search rate\n",
    "top_searchrate_counties = data.groupby('subgeography')['search_rate'].mean().nlargest(10)\n",
    "\n",
    "# Average hit rate for these counties\n",
    "avg_hitrate_top_stops = data[data['subgeography'].isin(top_stops_counties.index)]['hit_rate'].mean()\n",
    "avg_hitrate_top_stoprate = data[data['subgeography'].isin(top_stoprate_counties.index)]['hit_rate'].mean()\n",
    "avg_hitrate_top_searchrate = data[data['subgeography'].isin(top_searchrate_counties.index)]['hit_rate'].mean()\n",
    "\n",
    "(top_stops_counties, top_stoprate_counties, top_searchrate_counties, avg_hitrate_top_stops, avg_hitrate_top_stoprate, avg_hitrate_top_searchrate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of stop rate vs hit rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=data, x='stop_rate', y='hit_rate')\n",
    "plt.title('Stop Rate vs Hit Rate')\n",
    "plt.xlabel('Stop Rate')\n",
    "plt.ylabel('Hit Rate')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "stop_hit_corr = data[['stop_rate', 'hit_rate']].corr()\n",
    "\n",
    "# Scatter plot of search rate vs hit rate\n",
    "plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Key Findings</h1>\n",
    "\n",
    "***States and Counties:***\n",
    "\n",
    "The dataset covers 21 unique states with 1062 unique subgeographies or counties.\n",
    "The county with the most records is Jefferson County.\n",
    "Missing Values:\n",
    "\n",
    "The fields search_rate, hit_rate, and inferred_threshold have many missing values, which may limit the insights that can be derived from these fields.\n",
    "\n",
    "\n",
    "***Stops by Race:***\n",
    "\n",
    "The most stops were made for individuals identified as 'white', followed by 'hispanic' and 'black'.\n",
    "\n",
    "\n",
    "***Stop Rate by Race:***\n",
    "\n",
    "The median stop rate appears to be highest for 'black', followed by 'hispanic' and 'white'.\n",
    "\n",
    "***Search Rate by Race:***\n",
    "\n",
    "The median search rate appears to be slightly higher for 'black' and 'hispanic' compared to 'white'.\n",
    "\n",
    "***Stops by State:***\n",
    "\n",
    "Some states have a significantly higher total number of stops per year than others.\n",
    "The state with the highest total number of stops per year appears to be California (CA), followed by Texas (TX) and Florida (FL).\n",
    "\n",
    "\n",
    "***Rate Variations across States:***\n",
    "\n",
    "The stop rate and search rate vary considerably across states.\n",
    "The states with the highest average stop rate and search rate are not necessarily the ones with the highest total number of stops.\n",
    "\n",
    "***Correlation between Rates:***\n",
    "\n",
    "There is no strong linear relationship between the stop rate and the search rate or between the stop rate and the hit rate.\n",
    "The same is true for the search rate and the hit rate.\n",
    "\n",
    "***Stops by County:***\n",
    "\n",
    "The counties with the highest total number of stops per year include Los Angeles County, San Diego County, Orange County, San Bernardino County, and Alameda County, among others.\n",
    "The average hit rate for these counties is approximately 0.25.\n",
    "Rate Variations across Counties:\n",
    "\n",
    "The counties with the highest average stop rate and search rate do not necessarily align with the counties with the highest total number of stops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "numerical_cols = ['search_rate', 'stop_rate', 'hit_rate', 'inferred_threshold', 'stops_per_year', 'stop_rate_n']\n",
    "categorical_cols = ['subject_race', 'state', 'subgeography']\n",
    "\n",
    "# Preprocessing for numerical columns (fill missing values with median)\n",
    "num_transformer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Preprocessing for categorical columns (fill missing values with most frequent value and then one-hot encode)\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, numerical_cols),\n",
    "        ('cat', cat_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define your target variable and features\n",
    "# Define your target variable and features\n",
    "y = data['subject_race']  # replace 'subject_race' with the name of your target column\n",
    "X = data.drop('subject_race', axis=1)  # replace 'subject_race' with the name of your target column\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "numerical_cols = ['search_rate', 'hit_rate', 'inferred_threshold', 'stops_per_year']\n",
    "categorical_cols = ['subject_race', 'state', 'subgeography']\n",
    "\n",
    "# Preprocessing for numerical columns (fill missing values with median)\n",
    "num_transformer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Preprocessing for categorical columns (fill missing values with most frequent value and then one-hot encode)\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, numerical_cols),\n",
    "        ('cat', cat_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define your target variable and features\n",
    "y = data['stop_rate']  # replace 'stop_rate' with the name of your target column\n",
    "X = data.drop('stop_rate', axis=1)  # replace 'stop_rate' with the name of your target column\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "score = my_pipeline.score(X_test, y_test)\n",
    "print(f'Random Forest Regressor R^2 score: {score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set the MLflow tracking server to a backend\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')  # replace 'http://my-server:5000' with your backend server\n",
    "\n",
    "# Start a new MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Preprocessing of training data, fit model \n",
    "    my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Preprocessing of validation data, get predictions\n",
    "    preds = my_pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(my_pipeline, \"model\")\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "    # Log parameter\n",
    "    mlflow.log_param(\"model_type\", \"Random Forest Regressor\")\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import Flow, task, context, logging, flow\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "@task\n",
    "def load_data():\n",
    "    # Load your data here\n",
    "    # X_train, X_test, y_train, y_test should be defined here\n",
    "    return {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}\n",
    "\n",
    "@task\n",
    "def train_model(data):\n",
    "    my_pipeline.fit(data['X_train'], data['y_train'])\n",
    "    return my_pipeline\n",
    "\n",
    "@task\n",
    "def make_predictions(model, data):\n",
    "    preds = model.predict(data['X_test'])\n",
    "    return preds\n",
    "\n",
    "@task\n",
    "def calculate_metrics(data, preds):\n",
    "    rmse = np.sqrt(mean_squared_error(data['y_test'], preds))\n",
    "    mae = mean_absolute_error(data['y_test'], preds)\n",
    "    r2 = r2_score(data['y_test'], preds)\n",
    "    return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "@task\n",
    "def log_metrics(metrics):\n",
    "    logger = logging.get_run_logger()\n",
    "    logger.info(f\"RMSE: {metrics['rmse']}\")\n",
    "    logger.info(f\"MAE: {metrics['mae']}\")\n",
    "    logger.info(f\"R2: {metrics['r2']}\")\n",
    "    \n",
    "@flow(name=\"End-to-end Flow\")\n",
    "def run_flow_fn():\n",
    "    data = load_data()\n",
    "    model = train_model(data)\n",
    "    preds = make_predictions(model, data)\n",
    "    metrics = calculate_metrics(data, preds)\n",
    "    log = log_metrics(metrics)\n",
    "        \n",
    "# Call the function to run the flow\n",
    "run_flow_fn()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important IPs\n",
    "#Prometheus: localhost:9090 #Don't forget to cd into the Prometheus folder\n",
    "#MLFlow: localhost:5001\n",
    "#Grafana: localhost:3000\n",
    "\n",
    "#start the Grafana server\n",
    "#sudo service grafana-server start\n",
    "\n",
    "#start the Prometheus server\n",
    "#./prometheus --config.file=prometheus.yaml\n",
    "\n",
    "#Starts the Prefect Server\n",
    "#prefect server start\n",
    "\n",
    "\n",
    "#Gradio IP\n",
    "#http://127.0.0.1:7861/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Gradio App\n",
    "import gradio as gr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "numerical_cols = ['search_rate', 'hit_rate', 'inferred_threshold', 'stops_per_year']\n",
    "categorical_cols = ['subject_race', 'state', 'subgeography']\n",
    "\n",
    "# Preprocessing for numerical columns (fill missing values with median)\n",
    "num_transformer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Preprocessing for categorical columns (fill missing values with most frequent value and then one-hot encode)\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, numerical_cols),\n",
    "        ('cat', cat_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define your target variable and features\n",
    "y = data['stop_rate']  # replace 'stop_rate' with the name of your target column\n",
    "X = data.drop('stop_rate', axis=1)  # replace 'stop_rate' with the name of your target column\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "def predict(search_rate, hit_rate, inferred_threshold, stops_per_year, subject_race, state, subgeography):\n",
    "    df = pd.DataFrame({\n",
    "        'search_rate': [search_rate], \n",
    "        'hit_rate': [hit_rate],\n",
    "        'inferred_threshold': [inferred_threshold],\n",
    "        'stops_per_year': [stops_per_year],\n",
    "        'subject_race': [subject_race],\n",
    "        'state': [state],\n",
    "        'subgeography': [subgeography]\n",
    "    })\n",
    "    preds = my_pipeline.predict(df)\n",
    "    return preds[0]\n",
    "\n",
    "# Define the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=predict, \n",
    "    inputs=[\n",
    "        gr.inputs.Number(label=numerical_cols[0]),\n",
    "        gr.inputs.Number(label=numerical_cols[1]),\n",
    "        gr.inputs.Number(label=numerical_cols[2]),\n",
    "        gr.inputs.Number(label=numerical_cols[3]),\n",
    "        gr.inputs.Dropdown(choices=data[subject_race].unique().tolist(), label=categorical_cols[0]),\n",
    "        gr.inputs.Dropdown(choices=data[state].unique().tolist(), label=categorical_cols[1]),\n",
    "        gr.inputs.Dropdown(choices=data[subgeography].unique().tolist(), label=categorical_cols[2])\n",
    "    ], \n",
    "    outputs='number',\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch(share=)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
